{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "RNNs.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.0"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "shMl-W68iLyS",
        "colab_type": "text"
      },
      "source": [
        "# Задание 6: Рекуррентные нейронные сети (RNNs)\n",
        "\n",
        "Это задание адаптиповано из Deep NLP Course at ABBYY (https://github.com/DanAnastasyev/DeepNLP-Course) с разрешения автора - Даниила Анастасьева. Спасибо ему огромное!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "P59NYU98GCb9",
        "colab": {}
      },
      "source": [
        "!pip3 -qq install torch==0.4.1\n",
        "!pip3 -qq install bokeh==0.13.0\n",
        "!pip3 -qq install gensim==3.6.0\n",
        "!pip3 -qq install nltk\n",
        "!pip3 -qq install scikit-learn==0.20.2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "8sVtGHmA9aBM",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    from torch.cuda import FloatTensor, LongTensor\n",
        "else:\n",
        "    from torch import FloatTensor, LongTensor\n",
        "\n",
        "np.random.seed(42)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "-6CNKM3b4hT1"
      },
      "source": [
        "# Рекуррентные нейронные сети (RNNs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "O_XkoGNQUeGm"
      },
      "source": [
        "## POS Tagging"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "QFEtWrS_4rUs"
      },
      "source": [
        "Мы рассмотрим применение рекуррентных сетей к задаче sequence labeling (последняя картинка).\n",
        "\n",
        "![RNN types](http://karpathy.github.io/assets/rnn/diags.jpeg)\n",
        "\n",
        "*From [The Unreasonable Effectiveness of Recurrent Neural Networks](http://karpathy.github.io/2015/05/21/rnn-effectiveness/)*\n",
        "\n",
        "Самые популярные примеры для такой постановки задачи - Part-of-Speech Tagging и Named Entity Recognition.\n",
        "\n",
        "Мы порешаем сейчас POS Tagging для английского.\n",
        "\n",
        "Будем работать с таким набором тегов:\n",
        "- ADJ - adjective (new, good, high, ...)\n",
        "- ADP - adposition (on, of, at, ...)\n",
        "- ADV - adverb (really, already, still, ...)\n",
        "- CONJ - conjunction (and, or, but, ...)\n",
        "- DET - determiner, article (the, a, some, ...)\n",
        "- NOUN - noun (year, home, costs, ...)\n",
        "- NUM - numeral (twenty-four, fourth, 1991, ...)\n",
        "- PRT - particle (at, on, out, ...)\n",
        "- PRON - pronoun (he, their, her, ...)\n",
        "- VERB - verb (is, say, told, ...)\n",
        "- . - punctuation marks (. , ;)\n",
        "- X - other (ersatz, esprit, dunno, ...)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "EPIkKdFlHB-X"
      },
      "source": [
        "Скачаем данные:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "TiA2dGmgF1rW",
        "outputId": "a21b3ca2-6f18-4aa9-d3dd-260d9c8dbcc7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        }
      },
      "source": [
        "import nltk\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "nltk.download('brown')\n",
        "nltk.download('universal_tagset')\n",
        "\n",
        "data = nltk.corpus.brown.tagged_sents(tagset='universal')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/brown.zip.\n",
            "[nltk_data] Downloading package universal_tagset to /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/universal_tagset.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "d93g_swyJA_V"
      },
      "source": [
        "Пример размеченного предложения:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "QstS4NO0L97c",
        "outputId": "f86b1f41-083c-4941-866c-ee7a83a5b5ae",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 437
        }
      },
      "source": [
        "for word, tag in data[0]:\n",
        "    print('{:15}\\t{}'.format(word, tag))"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The            \tDET\n",
            "Fulton         \tNOUN\n",
            "County         \tNOUN\n",
            "Grand          \tADJ\n",
            "Jury           \tNOUN\n",
            "said           \tVERB\n",
            "Friday         \tNOUN\n",
            "an             \tDET\n",
            "investigation  \tNOUN\n",
            "of             \tADP\n",
            "Atlanta's      \tNOUN\n",
            "recent         \tADJ\n",
            "primary        \tNOUN\n",
            "election       \tNOUN\n",
            "produced       \tVERB\n",
            "``             \t.\n",
            "no             \tDET\n",
            "evidence       \tNOUN\n",
            "''             \t.\n",
            "that           \tADP\n",
            "any            \tDET\n",
            "irregularities \tNOUN\n",
            "took           \tVERB\n",
            "place          \tNOUN\n",
            ".              \t.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "epdW8u_YXcAv"
      },
      "source": [
        "Построим разбиение на train/val/test - наконец-то, всё как у нормальных людей.\n",
        "\n",
        "На train будем учиться, по val - подбирать параметры и делать всякие early stopping, а на test - принимать модель по ее финальному качеству."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "xTai8Ta0lgwL",
        "outputId": "2cbd7287-ca9d-427c-b642-74c5037af540",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        }
      },
      "source": [
        "train_data, test_data = train_test_split(data, test_size=0.25, random_state=42)\n",
        "train_data, val_data = train_test_split(train_data, test_size=0.15, random_state=42)\n",
        "\n",
        "print('Words count in train set:', sum(len(sent) for sent in train_data))\n",
        "print('Words count in val set:', sum(len(sent) for sent in val_data))\n",
        "print('Words count in test set:', sum(len(sent) for sent in test_data))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Words count in train set: 739769\n",
            "Words count in val set: 130954\n",
            "Words count in test set: 290469\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "eChdLNGtXyP0"
      },
      "source": [
        "Построим маппинги из слов в индекс и из тега в индекс:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "pCjwwDs6Zq9x",
        "outputId": "083291a1-5fa6-4b6a-f587-750ee66eea01",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "words = {word for sample in train_data for word, tag in sample}\n",
        "word2ind = {word: ind + 1 for ind, word in enumerate(words)}\n",
        "word2ind['<pad>'] = 0\n",
        "\n",
        "tags = {tag for sample in train_data for word, tag in sample}\n",
        "tag2ind = {tag: ind + 1 for ind, tag in enumerate(tags)}\n",
        "tag2ind['<pad>'] = 0\n",
        "\n",
        "print('Unique words in train = {}. Tags = {}'.format(len(word2ind), tags))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Unique words in train = 45441. Tags = {'CONJ', 'PRT', 'NOUN', 'ADV', '.', 'ADJ', 'ADP', 'VERB', 'X', 'DET', 'PRON', 'NUM'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "URC1B2nvPGFt",
        "outputId": "3f83e6ef-4296-4916-b782-4042ef364b32",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        }
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "from collections import Counter\n",
        "\n",
        "tag_distribution = Counter(tag for sample in train_data for _, tag in sample)\n",
        "tag_distribution = [tag_distribution[tag] for tag in tags]\n",
        "\n",
        "plt.figure(figsize=(10, 5))\n",
        "\n",
        "bar_width = 0.35\n",
        "plt.bar(np.arange(len(tags)), tag_distribution, bar_width, align='center', alpha=0.5)\n",
        "plt.xticks(np.arange(len(tags)), tags)\n",
        "    \n",
        "plt.show()"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAmwAAAEyCAYAAABH+Yw/AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nSUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 720x360 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "gArQwbzWWkgi"
      },
      "source": [
        "## Бейзлайн\n",
        "\n",
        "Какой самый простой теггер можно придумать? Давайте просто запоминать, какие теги самые вероятные для слова (или для последовательности):\n",
        "\n",
        "![tag-context](https://www.nltk.org/images/tag-context.png)  \n",
        "*From [Categorizing and Tagging Words, nltk](https://www.nltk.org/book/ch05.html)*\n",
        "\n",
        "На картинке показано, что для предсказания $t_n$ используются два предыдущих предсказанных тега + текущее слово. По корпусу считаются вероятность для $P(t_n| w_n, t_{n-1}, t_{n-2})$, выбирается тег с максимальной вероятностью.\n",
        "\n",
        "Более аккуратно такая идея реализована в Hidden Markov Models: по тренировочному корпусу вычисляются вероятности $P(w_n| t_n), P(t_n|t_{n-1}, t_{n-2})$ и максимизируется их произведение.\n",
        "\n",
        "Простейший вариант - униграммная модель, учитывающая только слово:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "5rWmSToIaeAo",
        "outputId": "05831d03-97fe-43d5-c1f2-078126f7fda0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import nltk\n",
        "\n",
        "default_tagger = nltk.DefaultTagger('NN')\n",
        "\n",
        "unigram_tagger = nltk.UnigramTagger(train_data, backoff=default_tagger)\n",
        "print('Accuracy of unigram tagger = {:.2%}'.format(unigram_tagger.evaluate(test_data)))"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy of unigram tagger = 92.62%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "07Ymb_MkbWsF"
      },
      "source": [
        "Добавим вероятности переходов:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "vjz_Rk0bbMyH",
        "outputId": "dc41fa67-3c9d-459a-f49a-c8d087aa1794",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "bigram_tagger = nltk.BigramTagger(train_data, backoff=unigram_tagger)\n",
        "print('Accuracy of bigram tagger = {:.2%}'.format(bigram_tagger.evaluate(test_data)))"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy of bigram tagger = 93.42%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "uWMw6QHvbaDd"
      },
      "source": [
        "Обратите внимание, что `backoff` важен:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "8XCuxEBVbOY_",
        "outputId": "7956ad67-a216-4785-bf21-ec858512d621",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "trigram_tagger = nltk.TrigramTagger(train_data)\n",
        "print('Accuracy of trigram tagger = {:.2%}'.format(trigram_tagger.evaluate(test_data)))"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy of trigram tagger = 23.33%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "4t3xyYd__8d-"
      },
      "source": [
        "## Увеличиваем контекст с рекуррентными сетями\n",
        "\n",
        "Униграмная модель работает на удивление хорошо, но мы же собрались учить сеточки.\n",
        "\n",
        "Омонимия - основная причина, почему униграмная модель плоха:  \n",
        "*“he cashed a check at the **bank**”*  \n",
        "vs  \n",
        "*“he sat on the **bank** of the river”*\n",
        "\n",
        "Поэтому нам очень полезно учитывать контекст при предсказании тега.\n",
        "\n",
        "Воспользуемся LSTM - он умеет работать с контекстом очень даже хорошо:\n",
        "\n",
        "![](https://image.ibb.co/kgmoff/Baseline-Tagger.png)\n",
        "\n",
        "Синим показано выделение фичей из слова, LSTM оранжевенький - он строит эмбеддинги слов с учетом контекста, а дальше зелененькая логистическая регрессия делает предсказания тегов."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "RtRbz1SwgEqc",
        "colab": {}
      },
      "source": [
        "def convert_data(data, word2ind, tag2ind):\n",
        "    X = [[word2ind.get(word, 0) for word, _ in sample] for sample in data]\n",
        "    y = [[tag2ind[tag] for _, tag in sample] for sample in data]\n",
        "    \n",
        "    return X, y\n",
        "\n",
        "X_train, y_train = convert_data(train_data, word2ind, tag2ind)\n",
        "X_val, y_val = convert_data(val_data, word2ind, tag2ind)\n",
        "X_test, y_test = convert_data(test_data, word2ind, tag2ind)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "DhsTKZalfih6",
        "colab": {}
      },
      "source": [
        "def iterate_batches(data, batch_size):\n",
        "    X, y = data\n",
        "    n_samples = len(X)\n",
        "\n",
        "    indices = np.arange(n_samples)\n",
        "    np.random.shuffle(indices)\n",
        "    \n",
        "    for start in range(0, n_samples, batch_size):\n",
        "        end = min(start + batch_size, n_samples)\n",
        "        \n",
        "        batch_indices = indices[start:end]\n",
        "        \n",
        "        max_sent_len = max(len(X[ind]) for ind in batch_indices)\n",
        "        X_batch = np.zeros((max_sent_len, len(batch_indices)))\n",
        "        y_batch = np.zeros((max_sent_len, len(batch_indices)))\n",
        "        \n",
        "        for batch_ind, sample_ind in enumerate(batch_indices):\n",
        "            X_batch[:len(X[sample_ind]), batch_ind] = X[sample_ind]\n",
        "            y_batch[:len(y[sample_ind]), batch_ind] = y[sample_ind]\n",
        "            \n",
        "        yield X_batch, y_batch"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "l4XsRII5kW5x",
        "outputId": "83017f8e-28ba-42a6-fa5a-e6662995f481",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "X_batch, y_batch = next(iterate_batches((X_train, y_train), 4))\n",
        "\n",
        "X_batch.shape, y_batch.shape"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((32, 4), (32, 4))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "C5I9E9P6eFYv"
      },
      "source": [
        "**Задание** Реализуйте `LSTMTagger`:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "WVEHju54d68T",
        "colab": {}
      },
      "source": [
        "class LSTMTagger(nn.Module):\n",
        "    def __init__(self, vocab_size, tagset_size, word_emb_dim=100, lstm_hidden_dim=128, lstm_layers_count=1):\n",
        "        super().__init__()\n",
        "        self.word_embedding = nn.Embedding(vocab_size, word_emb_dim)\n",
        "        self.lstm = nn.LSTM(word_emb_dim, lstm_hidden_dim, lstm_layers_count)\n",
        "        self.fc = nn.Linear(lstm_hidden_dim, tagset_size)\n",
        "    \n",
        "    def forward(self, inputs):\n",
        "        embeddings = self.word_embedding(inputs)\n",
        "        hidden, _  = self.lstm(embeddings)\n",
        "        prediction = self.fc(hidden)\n",
        "        \n",
        "        return prediction"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "q_HA8zyheYGH"
      },
      "source": [
        "**Задание** Научитесь считать accuracy и loss (а заодно проверьте, что модель работает)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "jbrxsZ2mehWB",
        "outputId": "96664e4c-9777-43cf-8c81-45960988cbe0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "model = LSTMTagger(\n",
        "    vocab_size=len(word2ind),\n",
        "    tagset_size=len(tag2ind)\n",
        ")\n",
        "\n",
        "X_batch = torch.LongTensor(X_batch)\n",
        "y_batch = torch.LongTensor(y_batch)\n",
        "\n",
        "model.eval()\n",
        "logits = model(X_batch)\n",
        "_, prediction = torch.max(logits, 2)\n",
        "accuracy = float(torch.sum(prediction == y_batch)) / y_batch.nelement()\n",
        "\n",
        "print(\"Точность = {}\".format(accuracy))"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Точность = 0.0390625\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "GMUyUm1hgpe3",
        "outputId": "ea67c268-0a7c-48d3-dd55-9d976e18a1a9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "loss = criterion(logits.view(-1, 13), y_batch.view(-1))\n",
        "print(\"Loss = {}\".format(loss.item()))"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loss = 2.549159049987793\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "nSgV3NPUpcjH"
      },
      "source": [
        "**Задание** Вставьте эти вычисление в функцию:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "FprPQ0gllo7b",
        "colab": {}
      },
      "source": [
        "import math\n",
        "from tqdm import tqdm\n",
        "\n",
        "\n",
        "def do_epoch(model, criterion, data, batch_size, optimizer=None, name=None):\n",
        "    epoch_loss = 0\n",
        "    correct_count = 0\n",
        "    sum_count = 0\n",
        "    \n",
        "    is_train = not optimizer is None\n",
        "    name = name or ''\n",
        "    model.train(is_train)\n",
        "    \n",
        "    batches_count = math.ceil(len(data[0]) / batch_size)\n",
        "    \n",
        "    with torch.autograd.set_grad_enabled(is_train):\n",
        "        with tqdm(total=batches_count) as progress_bar:\n",
        "            for i, (X_batch, y_batch) in enumerate(iterate_batches(data, batch_size)):\n",
        "                X_batch, y_batch = LongTensor(X_batch), LongTensor(y_batch)\n",
        "                logits = model(X_batch)\n",
        "\n",
        "                loss = criterion(logits.view(-1, 13), y_batch.view(-1))\n",
        "\n",
        "                epoch_loss += loss.item()\n",
        "\n",
        "                if optimizer:\n",
        "                    optimizer.zero_grad()\n",
        "                    loss.backward()\n",
        "                    optimizer.step()\n",
        "                \n",
        "                _, prediction = torch.max(logits, 2)\n",
        "                \n",
        "                prediction = prediction.view(-1)\n",
        "                y_batch = y_batch.view(-1)\n",
        "                \n",
        "                mask = torch.zeros_like(y_batch)\n",
        "                mask[y_batch != 0] = 1\n",
        "                \n",
        "                paddings = (mask == 0).sum()\n",
        "                \n",
        "                prediction = prediction * mask\n",
        "                \n",
        "                cur_correct_count, cur_sum_count = float(torch.sum(prediction == y_batch) - paddings), (y_batch.nelement() - paddings).item()\n",
        "                \n",
        "                correct_count += cur_correct_count\n",
        "                sum_count += cur_sum_count\n",
        "\n",
        "                progress_bar.update()\n",
        "                progress_bar.set_description('{:>5s} Loss = {:.5f}, Accuracy = {:.2%}'.format(\n",
        "                    name, loss.item(), cur_correct_count / cur_sum_count)\n",
        "                )\n",
        "                \n",
        "            progress_bar.set_description('{:>5s} Loss = {:.5f}, Accuracy = {:.2%}'.format(\n",
        "                name, epoch_loss / batches_count, correct_count / sum_count)\n",
        "            )\n",
        "\n",
        "    return epoch_loss / batches_count, correct_count / sum_count\n",
        "\n",
        "\n",
        "def fit(model, criterion, optimizer, train_data, epochs_count=1, batch_size=32,\n",
        "        val_data=None, val_batch_size=None):\n",
        "        \n",
        "    if not val_data is None and val_batch_size is None:\n",
        "        val_batch_size = batch_size\n",
        "        \n",
        "    for epoch in range(epochs_count):\n",
        "        name_prefix = '[{} / {}] '.format(epoch + 1, epochs_count)\n",
        "        train_loss, train_acc = do_epoch(model, criterion, train_data, batch_size, optimizer, name_prefix + 'Train:')\n",
        "        \n",
        "        if not val_data is None:\n",
        "            val_loss, val_acc = do_epoch(model, criterion, val_data, val_batch_size, None, name_prefix + '  Val:')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Pqfbeh1ltEYa",
        "outputId": "df0cc1d6-4084-48b9-9413-9c3dfe3facd8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1697
        }
      },
      "source": [
        "model = LSTMTagger(\n",
        "    vocab_size=len(word2ind),\n",
        "    tagset_size=len(tag2ind)\n",
        ").cuda()\n",
        "\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=0).cuda()\n",
        "optimizer = optim.Adam(model.parameters())\n",
        "\n",
        "fit(model, criterion, optimizer, train_data=(X_train, y_train), epochs_count=50,\n",
        "    batch_size=64, val_data=(X_val, y_val), val_batch_size=512)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1 / 50] Train: Loss = 0.68784, Accuracy = 78.12%: 100%|██████████| 572/572 [00:07<00:00, 76.48it/s]\n",
            "[1 / 50]   Val: Loss = 0.35349, Accuracy = 88.17%: 100%|██████████| 13/13 [00:00<00:00, 65.43it/s]\n",
            "[2 / 50] Train: Loss = 0.27573, Accuracy = 90.86%: 100%|██████████| 572/572 [00:06<00:00, 82.18it/s]\n",
            "[2 / 50]   Val: Loss = 0.23357, Accuracy = 92.24%: 100%|██████████| 13/13 [00:00<00:00, 74.20it/s]\n",
            "[3 / 50] Train: Loss = 0.18641, Accuracy = 93.84%: 100%|██████████| 572/572 [00:06<00:00, 84.09it/s]\n",
            "[3 / 50]   Val: Loss = 0.18587, Accuracy = 93.86%: 100%|██████████| 13/13 [00:00<00:00, 71.33it/s]\n",
            "[4 / 50] Train: Loss = 0.13946, Accuracy = 95.34%: 100%|██████████| 572/572 [00:06<00:00, 86.36it/s]\n",
            "[4 / 50]   Val: Loss = 0.16542, Accuracy = 94.56%: 100%|██████████| 13/13 [00:00<00:00, 75.09it/s]\n",
            "[5 / 50] Train: Loss = 0.10905, Accuracy = 96.33%: 100%|██████████| 572/572 [00:06<00:00, 84.56it/s]\n",
            "[5 / 50]   Val: Loss = 0.15548, Accuracy = 94.99%: 100%|██████████| 13/13 [00:00<00:00, 73.25it/s]\n",
            "[6 / 50] Train: Loss = 0.08700, Accuracy = 97.06%: 100%|██████████| 572/572 [00:06<00:00, 83.98it/s]\n",
            "[6 / 50]   Val: Loss = 0.15125, Accuracy = 95.25%: 100%|██████████| 13/13 [00:00<00:00, 73.87it/s]\n",
            "[7 / 50] Train: Loss = 0.07038, Accuracy = 97.61%: 100%|██████████| 572/572 [00:06<00:00, 83.47it/s]\n",
            "[7 / 50]   Val: Loss = 0.14501, Accuracy = 95.38%: 100%|██████████| 13/13 [00:00<00:00, 71.66it/s]\n",
            "[8 / 50] Train: Loss = 0.05762, Accuracy = 98.07%: 100%|██████████| 572/572 [00:06<00:00, 85.21it/s]\n",
            "[8 / 50]   Val: Loss = 0.14990, Accuracy = 95.46%: 100%|██████████| 13/13 [00:00<00:00, 78.93it/s]\n",
            "[9 / 50] Train: Loss = 0.04737, Accuracy = 98.41%: 100%|██████████| 572/572 [00:06<00:00, 84.59it/s]\n",
            "[9 / 50]   Val: Loss = 0.15532, Accuracy = 95.53%: 100%|██████████| 13/13 [00:00<00:00, 68.01it/s]\n",
            "[10 / 50] Train: Loss = 0.03890, Accuracy = 98.69%: 100%|██████████| 572/572 [00:06<00:00, 83.71it/s]\n",
            "[10 / 50]   Val: Loss = 0.16156, Accuracy = 95.50%: 100%|██████████| 13/13 [00:00<00:00, 72.50it/s]\n",
            "[11 / 50] Train: Loss = 0.03227, Accuracy = 98.93%: 100%|██████████| 572/572 [00:06<00:00, 83.94it/s]\n",
            "[11 / 50]   Val: Loss = 0.16619, Accuracy = 95.48%: 100%|██████████| 13/13 [00:00<00:00, 71.43it/s]\n",
            "[12 / 50] Train: Loss = 0.02668, Accuracy = 99.12%: 100%|██████████| 572/572 [00:06<00:00, 82.08it/s]\n",
            "[12 / 50]   Val: Loss = 0.17491, Accuracy = 95.42%: 100%|██████████| 13/13 [00:00<00:00, 68.52it/s]\n",
            "[13 / 50] Train: Loss = 0.02173, Accuracy = 99.31%: 100%|██████████| 572/572 [00:06<00:00, 81.97it/s]\n",
            "[13 / 50]   Val: Loss = 0.18165, Accuracy = 95.38%: 100%|██████████| 13/13 [00:00<00:00, 64.69it/s]\n",
            "[14 / 50] Train: Loss = 0.01798, Accuracy = 99.44%: 100%|██████████| 572/572 [00:06<00:00, 84.26it/s]\n",
            "[14 / 50]   Val: Loss = 0.19341, Accuracy = 95.37%: 100%|██████████| 13/13 [00:00<00:00, 73.09it/s]\n",
            "[15 / 50] Train: Loss = 0.01463, Accuracy = 99.56%: 100%|██████████| 572/572 [00:06<00:00, 84.49it/s]\n",
            "[15 / 50]   Val: Loss = 0.20481, Accuracy = 95.22%: 100%|██████████| 13/13 [00:00<00:00, 69.13it/s]\n",
            "[16 / 50] Train: Loss = 0.01223, Accuracy = 99.63%: 100%|██████████| 572/572 [00:06<00:00, 85.14it/s]\n",
            "[16 / 50]   Val: Loss = 0.21199, Accuracy = 95.11%: 100%|██████████| 13/13 [00:00<00:00, 74.10it/s]\n",
            "[17 / 50] Train: Loss = 0.01007, Accuracy = 99.70%: 100%|██████████| 572/572 [00:06<00:00, 86.24it/s]\n",
            "[17 / 50]   Val: Loss = 0.22075, Accuracy = 95.14%: 100%|██████████| 13/13 [00:00<00:00, 71.35it/s]\n",
            "[18 / 50] Train: Loss = 0.00871, Accuracy = 99.74%: 100%|██████████| 572/572 [00:06<00:00, 85.03it/s]\n",
            "[18 / 50]   Val: Loss = 0.23276, Accuracy = 95.06%: 100%|██████████| 13/13 [00:00<00:00, 70.31it/s]\n",
            "[19 / 50] Train: Loss = 0.00729, Accuracy = 99.78%: 100%|██████████| 572/572 [00:06<00:00, 84.07it/s]\n",
            "[19 / 50]   Val: Loss = 0.23917, Accuracy = 95.04%: 100%|██████████| 13/13 [00:00<00:00, 70.16it/s]\n",
            "[20 / 50] Train: Loss = 0.00692, Accuracy = 99.79%: 100%|██████████| 572/572 [00:06<00:00, 84.93it/s]\n",
            "[20 / 50]   Val: Loss = 0.24399, Accuracy = 95.08%: 100%|██████████| 13/13 [00:00<00:00, 72.34it/s]\n",
            "[21 / 50] Train: Loss = 0.00641, Accuracy = 99.80%: 100%|██████████| 572/572 [00:06<00:00, 82.03it/s]\n",
            "[21 / 50]   Val: Loss = 0.25233, Accuracy = 95.02%: 100%|██████████| 13/13 [00:00<00:00, 68.37it/s]\n",
            "[22 / 50] Train: Loss = 0.00600, Accuracy = 99.81%: 100%|██████████| 572/572 [00:06<00:00, 83.76it/s]\n",
            "[22 / 50]   Val: Loss = 0.25719, Accuracy = 95.07%: 100%|██████████| 13/13 [00:00<00:00, 70.60it/s]\n",
            "[23 / 50] Train: Loss = 0.00563, Accuracy = 99.82%: 100%|██████████| 572/572 [00:06<00:00, 83.75it/s]\n",
            "[23 / 50]   Val: Loss = 0.26738, Accuracy = 95.05%: 100%|██████████| 13/13 [00:00<00:00, 64.79it/s]\n",
            "[24 / 50] Train: Loss = 0.00582, Accuracy = 99.81%: 100%|██████████| 572/572 [00:07<00:00, 77.94it/s]\n",
            "[24 / 50]   Val: Loss = 0.27478, Accuracy = 95.04%: 100%|██████████| 13/13 [00:00<00:00, 67.25it/s]\n",
            "[25 / 50] Train: Loss = 0.00553, Accuracy = 99.82%: 100%|██████████| 572/572 [00:06<00:00, 85.38it/s]\n",
            "[25 / 50]   Val: Loss = 0.27538, Accuracy = 95.02%: 100%|██████████| 13/13 [00:00<00:00, 70.76it/s]\n",
            "[26 / 50] Train: Loss = 0.00526, Accuracy = 99.82%: 100%|██████████| 572/572 [00:06<00:00, 86.00it/s]\n",
            "[26 / 50]   Val: Loss = 0.28319, Accuracy = 94.98%: 100%|██████████| 13/13 [00:00<00:00, 67.22it/s]\n",
            "[27 / 50] Train: Loss = 0.00480, Accuracy = 99.83%: 100%|██████████| 572/572 [00:06<00:00, 87.25it/s] \n",
            "[27 / 50]   Val: Loss = 0.28726, Accuracy = 95.06%: 100%|██████████| 13/13 [00:00<00:00, 73.08it/s]\n",
            "[28 / 50] Train: Loss = 0.00474, Accuracy = 99.83%: 100%|██████████| 572/572 [00:06<00:00, 84.86it/s]\n",
            "[28 / 50]   Val: Loss = 0.28940, Accuracy = 95.06%: 100%|██████████| 13/13 [00:00<00:00, 68.38it/s]\n",
            "[29 / 50] Train: Loss = 0.00486, Accuracy = 99.83%: 100%|██████████| 572/572 [00:06<00:00, 85.92it/s]\n",
            "[29 / 50]   Val: Loss = 0.29813, Accuracy = 95.05%: 100%|██████████| 13/13 [00:00<00:00, 72.71it/s]\n",
            "[30 / 50] Train: Loss = 0.00515, Accuracy = 99.82%: 100%|██████████| 572/572 [00:06<00:00, 86.25it/s]\n",
            "[30 / 50]   Val: Loss = 0.28962, Accuracy = 95.00%: 100%|██████████| 13/13 [00:00<00:00, 73.37it/s]\n",
            "[31 / 50] Train: Loss = 0.00461, Accuracy = 99.83%: 100%|██████████| 572/572 [00:06<00:00, 84.63it/s]\n",
            "[31 / 50]   Val: Loss = 0.29907, Accuracy = 95.00%: 100%|██████████| 13/13 [00:00<00:00, 71.56it/s]\n",
            "[32 / 50] Train: Loss = 0.00445, Accuracy = 99.84%: 100%|██████████| 572/572 [00:06<00:00, 85.47it/s]\n",
            "[32 / 50]   Val: Loss = 0.30157, Accuracy = 95.04%: 100%|██████████| 13/13 [00:00<00:00, 71.99it/s]\n",
            "[33 / 50] Train: Loss = 0.00447, Accuracy = 99.83%: 100%|██████████| 572/572 [00:06<00:00, 85.33it/s]\n",
            "[33 / 50]   Val: Loss = 0.30800, Accuracy = 95.04%: 100%|██████████| 13/13 [00:00<00:00, 77.50it/s]\n",
            "[34 / 50] Train: Loss = 0.00445, Accuracy = 99.83%: 100%|██████████| 572/572 [00:06<00:00, 87.17it/s]\n",
            "[34 / 50]   Val: Loss = 0.30246, Accuracy = 95.04%: 100%|██████████| 13/13 [00:00<00:00, 73.04it/s]\n",
            "[35 / 50] Train: Loss = 0.00455, Accuracy = 99.83%: 100%|██████████| 572/572 [00:07<00:00, 81.53it/s]\n",
            "[35 / 50]   Val: Loss = 0.30540, Accuracy = 95.11%: 100%|██████████| 13/13 [00:00<00:00, 62.86it/s]\n",
            "[36 / 50] Train: Loss = 0.00486, Accuracy = 99.82%: 100%|██████████| 572/572 [00:07<00:00, 79.71it/s]\n",
            "[36 / 50]   Val: Loss = 0.30169, Accuracy = 94.99%: 100%|██████████| 13/13 [00:00<00:00, 69.44it/s]\n",
            "[37 / 50] Train: Loss = 0.00474, Accuracy = 99.82%: 100%|██████████| 572/572 [00:06<00:00, 84.59it/s]\n",
            "[37 / 50]   Val: Loss = 0.30522, Accuracy = 95.10%: 100%|██████████| 13/13 [00:00<00:00, 75.40it/s]\n",
            "[38 / 50] Train: Loss = 0.00432, Accuracy = 99.84%: 100%|██████████| 572/572 [00:07<00:00, 78.00it/s]\n",
            "[38 / 50]   Val: Loss = 0.30584, Accuracy = 95.02%: 100%|██████████| 13/13 [00:00<00:00, 66.35it/s]\n",
            "[39 / 50] Train: Loss = 0.00408, Accuracy = 99.84%: 100%|██████████| 572/572 [00:07<00:00, 81.16it/s]\n",
            "[39 / 50]   Val: Loss = 0.30731, Accuracy = 95.13%: 100%|██████████| 13/13 [00:00<00:00, 69.99it/s]\n",
            "[40 / 50] Train: Loss = 0.00391, Accuracy = 99.84%: 100%|██████████| 572/572 [00:06<00:00, 85.29it/s]\n",
            "[40 / 50]   Val: Loss = 0.31467, Accuracy = 95.12%: 100%|██████████| 13/13 [00:00<00:00, 72.89it/s]\n",
            "[41 / 50] Train: Loss = 0.00399, Accuracy = 99.84%: 100%|██████████| 572/572 [00:06<00:00, 80.58it/s] \n",
            "[41 / 50]   Val: Loss = 0.30521, Accuracy = 95.13%: 100%|██████████| 13/13 [00:00<00:00, 73.54it/s]\n",
            "[42 / 50] Train: Loss = 0.00462, Accuracy = 99.82%: 100%|██████████| 572/572 [00:06<00:00, 87.03it/s]\n",
            "[42 / 50]   Val: Loss = 0.31217, Accuracy = 95.12%: 100%|██████████| 13/13 [00:00<00:00, 72.13it/s]\n",
            "[43 / 50] Train: Loss = 0.00452, Accuracy = 99.83%: 100%|██████████| 572/572 [00:06<00:00, 87.03it/s]\n",
            "[43 / 50]   Val: Loss = 0.31316, Accuracy = 95.08%: 100%|██████████| 13/13 [00:00<00:00, 67.94it/s]\n",
            "[44 / 50] Train: Loss = 0.00427, Accuracy = 99.83%: 100%|██████████| 572/572 [00:06<00:00, 84.75it/s]\n",
            "[44 / 50]   Val: Loss = 0.31612, Accuracy = 95.14%: 100%|██████████| 13/13 [00:00<00:00, 67.17it/s]\n",
            "[45 / 50] Train: Loss = 0.00390, Accuracy = 99.85%: 100%|██████████| 572/572 [00:06<00:00, 84.89it/s]\n",
            "[45 / 50]   Val: Loss = 0.31554, Accuracy = 95.20%: 100%|██████████| 13/13 [00:00<00:00, 67.73it/s]\n",
            "[46 / 50] Train: Loss = 0.00376, Accuracy = 99.84%: 100%|██████████| 572/572 [00:06<00:00, 84.61it/s]\n",
            "[46 / 50]   Val: Loss = 0.31894, Accuracy = 95.13%: 100%|██████████| 13/13 [00:00<00:00, 69.67it/s]\n",
            "[47 / 50] Train: Loss = 0.00376, Accuracy = 99.85%: 100%|██████████| 572/572 [00:07<00:00, 79.82it/s]\n",
            "[47 / 50]   Val: Loss = 0.31746, Accuracy = 95.17%: 100%|██████████| 13/13 [00:00<00:00, 64.57it/s]\n",
            "[48 / 50] Train: Loss = 0.00379, Accuracy = 99.84%: 100%|██████████| 572/572 [00:06<00:00, 84.38it/s]\n",
            "[48 / 50]   Val: Loss = 0.31809, Accuracy = 95.10%: 100%|██████████| 13/13 [00:00<00:00, 69.18it/s]\n",
            "[49 / 50] Train: Loss = 0.00398, Accuracy = 99.84%: 100%|██████████| 572/572 [00:06<00:00, 85.40it/s]\n",
            "[49 / 50]   Val: Loss = 0.32814, Accuracy = 95.00%: 100%|██████████| 13/13 [00:00<00:00, 71.96it/s]\n",
            "[50 / 50] Train: Loss = 0.00630, Accuracy = 99.77%: 100%|██████████| 572/572 [00:06<00:00, 85.40it/s]\n",
            "[50 / 50]   Val: Loss = 0.32156, Accuracy = 95.08%: 100%|██████████| 13/13 [00:00<00:00, 72.15it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "m0qGetIhfUE5"
      },
      "source": [
        "### Masking\n",
        "\n",
        "**Задание** Проверьте себя - не считаете ли вы потери и accuracy на паддингах - очень легко получить высокое качество за счет этого.\n",
        "\n",
        "У функции потерь есть параметр `ignore_index`, для таких целей. Для accuracy нужно использовать маскинг - умножение на маску из нулей и единиц, где нули на позициях паддингов (а потом усреднение по ненулевым позициям в маске)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "nAfV2dEOfHo5"
      },
      "source": [
        "**Задание** Посчитайте качество модели на тесте. Ожидается результат лучше бейзлайна!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "98wr38_rw55D",
        "outputId": "ed00c0ff-07e0-4c86-c241-ad832f32775a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "test_loss, test_acc = do_epoch(model, criterion, data=(X_test, y_test), batch_size=512)\n",
        "print(\"Test accuracy{}\".format(test_acc))"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "      Loss = 0.32209, Accuracy = 95.15%: 100%|██████████| 28/28 [00:00<00:00, 52.87it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Test accuracy0.9515025699816504\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "PXUTSFaEHbDG"
      },
      "source": [
        "### Bidirectional LSTM\n",
        "\n",
        "Благодаря BiLSTM можно использовать сразу оба контеста при предсказании тега слова. Т.е. для каждого токена $w_i$ forward LSTM будет выдавать представление $\\mathbf{f_i} \\sim (w_1, \\ldots, w_i)$ - построенное по всему левому контексту - и $\\mathbf{b_i} \\sim (w_n, \\ldots, w_i)$ - представление правого контекста. Их конкатенация автоматически захватит весь доступный контекст слова: $\\mathbf{h_i} = [\\mathbf{f_i}, \\mathbf{b_i}] \\sim (w_1, \\ldots, w_n)$.\n",
        "\n",
        "![BiLSTM](https://www.researchgate.net/profile/Wang_Ling/publication/280912217/figure/fig2/AS:391505383575555@1470353565299/Illustration-of-our-neural-network-for-POS-tagging.png)  \n",
        "*From [Finding Function in Form: Compositional Character Models for Open Vocabulary Word Representation](https://arxiv.org/abs/1508.02096)*\n",
        "\n",
        "**Задание** Добавьте Bidirectional LSTM."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ow24JbXCiLzU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class BidirectLSTMTagger(nn.Module):\n",
        "    def __init__(self, vocab_size, tagset_size, word_emb_dim=100, lstm_hidden_dim=128, lstm_layers_count=1):\n",
        "        super().__init__()\n",
        "        self.word_embedding = nn.Embedding(vocab_size, word_emb_dim)\n",
        "        self.lstm = nn.LSTM(word_emb_dim, lstm_hidden_dim, lstm_layers_count, bidirectional=True)\n",
        "        self.fc = nn.Linear(2 * lstm_hidden_dim, tagset_size)\n",
        "    \n",
        "    def forward(self, inputs):\n",
        "        embeddings = self.word_embedding(inputs)\n",
        "        hidden, _  = self.lstm(embeddings)\n",
        "        prediction = self.fc(hidden)\n",
        "        \n",
        "        return prediction"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FN_u3EKp9N24",
        "colab_type": "code",
        "outputId": "cab9ba00-5a3e-43a9-b89f-48dabd570c9e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 521
        }
      },
      "source": [
        "bidir_model = BidirectLSTMTagger(\n",
        "    vocab_size=len(word2ind),\n",
        "    tagset_size=len(tag2ind)\n",
        ").cuda()\n",
        "\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=0).cuda()\n",
        "optimizer = optim.Adam(bidir_model.parameters())\n",
        "\n",
        "fit(bidir_model, criterion, optimizer, train_data=(X_train, y_train), epochs_count=15,\n",
        "    batch_size=64, val_data=(X_val, y_val), val_batch_size=512)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1 / 15] Train: Loss = 0.55571, Accuracy = 82.55%: 100%|██████████| 572/572 [00:08<00:00, 64.93it/s]\n",
            "[1 / 15]   Val: Loss = 0.27717, Accuracy = 90.99%: 100%|██████████| 13/13 [00:00<00:00, 54.04it/s]\n",
            "[2 / 15] Train: Loss = 0.20478, Accuracy = 93.47%: 100%|██████████| 572/572 [00:08<00:00, 65.03it/s]\n",
            "[2 / 15]   Val: Loss = 0.18588, Accuracy = 93.92%: 100%|██████████| 13/13 [00:00<00:00, 49.90it/s]\n",
            "[3 / 15] Train: Loss = 0.13017, Accuracy = 95.98%: 100%|██████████| 572/572 [00:09<00:00, 62.44it/s]\n",
            "[3 / 15]   Val: Loss = 0.14938, Accuracy = 95.08%: 100%|██████████| 13/13 [00:00<00:00, 51.64it/s]\n",
            "[4 / 15] Train: Loss = 0.08902, Accuracy = 97.28%: 100%|██████████| 572/572 [00:09<00:00, 61.28it/s]\n",
            "[4 / 15]   Val: Loss = 0.13319, Accuracy = 95.59%: 100%|██████████| 13/13 [00:00<00:00, 52.08it/s]\n",
            "[5 / 15] Train: Loss = 0.06172, Accuracy = 98.18%: 100%|██████████| 572/572 [00:08<00:00, 65.99it/s]\n",
            "[5 / 15]   Val: Loss = 0.12721, Accuracy = 95.84%: 100%|██████████| 13/13 [00:00<00:00, 54.38it/s]\n",
            "[6 / 15] Train: Loss = 0.04253, Accuracy = 98.77%: 100%|██████████| 572/572 [00:08<00:00, 65.08it/s]\n",
            "[6 / 15]   Val: Loss = 0.12770, Accuracy = 96.02%: 100%|██████████| 13/13 [00:00<00:00, 53.09it/s]\n",
            "[7 / 15] Train: Loss = 0.02854, Accuracy = 99.20%: 100%|██████████| 572/572 [00:08<00:00, 65.25it/s]\n",
            "[7 / 15]   Val: Loss = 0.12511, Accuracy = 96.29%: 100%|██████████| 13/13 [00:00<00:00, 55.51it/s]\n",
            "[8 / 15] Train: Loss = 0.01879, Accuracy = 99.51%: 100%|██████████| 572/572 [00:08<00:00, 64.16it/s]\n",
            "[8 / 15]   Val: Loss = 0.13100, Accuracy = 96.23%: 100%|██████████| 13/13 [00:00<00:00, 50.17it/s]\n",
            "[9 / 15] Train: Loss = 0.01210, Accuracy = 99.71%: 100%|██████████| 572/572 [00:09<00:00, 62.95it/s]\n",
            "[9 / 15]   Val: Loss = 0.14019, Accuracy = 96.22%: 100%|██████████| 13/13 [00:00<00:00, 54.39it/s]\n",
            "[10 / 15] Train: Loss = 0.00755, Accuracy = 99.84%: 100%|██████████| 572/572 [00:08<00:00, 64.67it/s]\n",
            "[10 / 15]   Val: Loss = 0.14976, Accuracy = 96.20%: 100%|██████████| 13/13 [00:00<00:00, 49.43it/s]\n",
            "[11 / 15] Train: Loss = 0.00470, Accuracy = 99.92%: 100%|██████████| 572/572 [00:08<00:00, 64.77it/s]\n",
            "[11 / 15]   Val: Loss = 0.15746, Accuracy = 96.22%: 100%|██████████| 13/13 [00:00<00:00, 54.54it/s]\n",
            "[12 / 15] Train: Loss = 0.00295, Accuracy = 99.96%: 100%|██████████| 572/572 [00:09<00:00, 61.44it/s]\n",
            "[12 / 15]   Val: Loss = 0.16051, Accuracy = 96.23%: 100%|██████████| 13/13 [00:00<00:00, 50.91it/s]\n",
            "[13 / 15] Train: Loss = 0.00195, Accuracy = 99.98%: 100%|██████████| 572/572 [00:09<00:00, 62.53it/s]\n",
            "[13 / 15]   Val: Loss = 0.16991, Accuracy = 96.23%: 100%|██████████| 13/13 [00:00<00:00, 53.33it/s]\n",
            "[14 / 15] Train: Loss = 0.00118, Accuracy = 99.99%: 100%|██████████| 572/572 [00:08<00:00, 64.77it/s]\n",
            "[14 / 15]   Val: Loss = 0.18035, Accuracy = 96.19%: 100%|██████████| 13/13 [00:00<00:00, 53.16it/s]\n",
            "[15 / 15] Train: Loss = 0.00095, Accuracy = 99.99%: 100%|██████████| 572/572 [00:08<00:00, 65.38it/s]\n",
            "[15 / 15]   Val: Loss = 0.18933, Accuracy = 96.17%: 100%|██████████| 13/13 [00:00<00:00, 54.10it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pIs1MO_4-DK2",
        "colab_type": "code",
        "outputId": "d4e16e3c-3e0c-4375-ecb1-669998fcc386",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        }
      },
      "source": [
        "test_loss, test_acc = do_epoch(bidir_model, criterion, data=(X_test, y_test), batch_size=512)\n",
        "\n",
        "print()\n",
        "print(\"Bidirectional LSTM test accuracy = {}\".format(test_acc))"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "      Loss = 0.18957, Accuracy = 96.16%: 100%|██████████| 28/28 [00:00<00:00, 45.78it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Bidirectional LSTM test accuracy = 0.9615828195091387\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ZTXmYGD_ANhm"
      },
      "source": [
        "### Предобученные эмбеддинги\n",
        "\n",
        "Мы знаем, какая клёвая вещь - предобученные эмбеддинги. При текущем размере обучающей выборки еще можно было учить их и с нуля - с меньшей было бы совсем плохо.\n",
        "\n",
        "Поэтому стандартный пайплайн - скачать эмбеддинги, засунуть их в сеточку. Запустим его:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "uZpY_Q1xZ18h",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "af224cbd-5c47-47a4-d36c-f29478a67ea5"
      },
      "source": [
        "import gensim.downloader as api\n",
        "\n",
        "w2v_model = api.load('glove-wiki-gigaword-100')"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[==================================================] 100.0% 128.1/128.1MB downloaded\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "KYogOoKlgtcf"
      },
      "source": [
        "Построим подматрицу для слов из нашей тренировочной выборки:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "VsCstxiO03oT",
        "outputId": "cdc726bb-d996-486b-b1b3-92d771e2a02e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "known_count = 0\n",
        "embeddings = np.zeros((len(word2ind), w2v_model.vectors.shape[1]))\n",
        "for word, ind in word2ind.items():\n",
        "    word = word.lower()\n",
        "    if word in w2v_model.vocab:\n",
        "        embeddings[ind] = w2v_model.get_vector(word)\n",
        "        known_count += 1\n",
        "        \n",
        "print('Know {} out of {} word embeddings'.format(known_count, len(word2ind)))"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Know 38736 out of 45441 word embeddings\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u3w1POABM57j",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "embeddings = torch.FloatTensor(embeddings)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "HcG7i-R8hbY3"
      },
      "source": [
        "**Задание** Сделайте модель с предобученной матрицей. Используйте `nn.Embedding.from_pretrained`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "LxaRBpQd0pat",
        "colab": {}
      },
      "source": [
        "class LSTMTaggerWithPretrainedEmbs(nn.Module):\n",
        "    def __init__(self, embedds, tagset_size, lstm_hidden_dim=64, lstm_layers_count=1):\n",
        "        super().__init__()\n",
        "        self.word_embedding = nn.Embedding.from_pretrained(embedds)\n",
        "        self.lstm = nn.LSTM(self.word_embedding.embedding_dim, lstm_hidden_dim, lstm_layers_count, bidirectional=True)\n",
        "        self.fc = nn.Linear(2 * lstm_hidden_dim, tagset_size)\n",
        "    \n",
        "    def forward(self, inputs):\n",
        "        word_embedded = self.word_embedding(inputs)\n",
        "        hidden, _  = self.lstm(word_embedded)\n",
        "        prediction = self.fc(hidden)\n",
        "        \n",
        "        return prediction"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "EBtI6BDE-Fc7",
        "outputId": "6409b4e0-4849-4227-9e20-99085e1e9b4f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 689
        }
      },
      "source": [
        "model2 = LSTMTaggerWithPretrainedEmbs(\n",
        "    embedds=embeddings,\n",
        "    tagset_size=len(tag2ind)\n",
        ").cuda()\n",
        "\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=0).cuda()\n",
        "optimizer = optim.Adam(model2.parameters())\n",
        "\n",
        "fit(model2, criterion, optimizer, train_data=(X_train, y_train), epochs_count=20,\n",
        "    batch_size=64, val_data=(X_val, y_val), val_batch_size=512)"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1 / 20] Train: Loss = 0.57805, Accuracy = 83.06%: 100%|██████████| 572/572 [00:07<00:00, 79.52it/s]\n",
            "[1 / 20]   Val: Loss = 0.25816, Accuracy = 92.35%: 100%|██████████| 13/13 [00:00<00:00, 61.86it/s]\n",
            "[2 / 20] Train: Loss = 0.19078, Accuracy = 94.35%: 100%|██████████| 572/572 [00:07<00:00, 80.02it/s]\n",
            "[2 / 20]   Val: Loss = 0.17642, Accuracy = 94.68%: 100%|██████████| 13/13 [00:00<00:00, 64.16it/s]\n",
            "[3 / 20] Train: Loss = 0.13724, Accuracy = 95.91%: 100%|██████████| 572/572 [00:07<00:00, 79.72it/s]\n",
            "[3 / 20]   Val: Loss = 0.14324, Accuracy = 95.62%: 100%|██████████| 13/13 [00:00<00:00, 57.29it/s]\n",
            "[4 / 20] Train: Loss = 0.11103, Accuracy = 96.64%: 100%|██████████| 572/572 [00:08<00:00, 71.34it/s]\n",
            "[4 / 20]   Val: Loss = 0.12544, Accuracy = 96.11%: 100%|██████████| 13/13 [00:00<00:00, 56.61it/s]\n",
            "[5 / 20] Train: Loss = 0.09535, Accuracy = 97.11%: 100%|██████████| 572/572 [00:07<00:00, 79.76it/s]\n",
            "[5 / 20]   Val: Loss = 0.11480, Accuracy = 96.36%: 100%|██████████| 13/13 [00:00<00:00, 65.73it/s]\n",
            "[6 / 20] Train: Loss = 0.08466, Accuracy = 97.41%: 100%|██████████| 572/572 [00:06<00:00, 82.42it/s]\n",
            "[6 / 20]   Val: Loss = 0.10758, Accuracy = 96.57%: 100%|██████████| 13/13 [00:00<00:00, 63.01it/s]\n",
            "[7 / 20] Train: Loss = 0.07701, Accuracy = 97.62%: 100%|██████████| 572/572 [00:07<00:00, 81.14it/s]\n",
            "[7 / 20]   Val: Loss = 0.10268, Accuracy = 96.70%: 100%|██████████| 13/13 [00:00<00:00, 62.20it/s]\n",
            "[8 / 20] Train: Loss = 0.07086, Accuracy = 97.81%: 100%|██████████| 572/572 [00:06<00:00, 81.92it/s]\n",
            "[8 / 20]   Val: Loss = 0.09952, Accuracy = 96.83%: 100%|██████████| 13/13 [00:00<00:00, 63.65it/s]\n",
            "[9 / 20] Train: Loss = 0.06569, Accuracy = 97.96%: 100%|██████████| 572/572 [00:06<00:00, 82.74it/s]\n",
            "[9 / 20]   Val: Loss = 0.09751, Accuracy = 96.91%: 100%|██████████| 13/13 [00:00<00:00, 65.79it/s]\n",
            "[10 / 20] Train: Loss = 0.06131, Accuracy = 98.08%: 100%|██████████| 572/572 [00:06<00:00, 83.97it/s]\n",
            "[10 / 20]   Val: Loss = 0.09488, Accuracy = 96.92%: 100%|██████████| 13/13 [00:00<00:00, 62.51it/s]\n",
            "[11 / 20] Train: Loss = 0.05768, Accuracy = 98.20%: 100%|██████████| 572/572 [00:07<00:00, 81.06it/s]\n",
            "[11 / 20]   Val: Loss = 0.09517, Accuracy = 96.95%: 100%|██████████| 13/13 [00:00<00:00, 61.84it/s]\n",
            "[12 / 20] Train: Loss = 0.05454, Accuracy = 98.28%: 100%|██████████| 572/572 [00:07<00:00, 75.06it/s]\n",
            "[12 / 20]   Val: Loss = 0.09280, Accuracy = 96.96%: 100%|██████████| 13/13 [00:00<00:00, 55.69it/s]\n",
            "[13 / 20] Train: Loss = 0.05188, Accuracy = 98.36%: 100%|██████████| 572/572 [00:07<00:00, 79.76it/s]\n",
            "[13 / 20]   Val: Loss = 0.09212, Accuracy = 97.04%: 100%|██████████| 13/13 [00:00<00:00, 63.43it/s]\n",
            "[14 / 20] Train: Loss = 0.04925, Accuracy = 98.44%: 100%|██████████| 572/572 [00:07<00:00, 80.55it/s]\n",
            "[14 / 20]   Val: Loss = 0.09233, Accuracy = 97.04%: 100%|██████████| 13/13 [00:00<00:00, 61.54it/s]\n",
            "[15 / 20] Train: Loss = 0.04669, Accuracy = 98.53%: 100%|██████████| 572/572 [00:08<00:00, 71.29it/s]\n",
            "[15 / 20]   Val: Loss = 0.09189, Accuracy = 97.05%: 100%|██████████| 13/13 [00:00<00:00, 60.12it/s]\n",
            "[16 / 20] Train: Loss = 0.04450, Accuracy = 98.59%: 100%|██████████| 572/572 [00:06<00:00, 81.81it/s]\n",
            "[16 / 20]   Val: Loss = 0.09180, Accuracy = 97.08%: 100%|██████████| 13/13 [00:00<00:00, 63.45it/s]\n",
            "[17 / 20] Train: Loss = 0.04263, Accuracy = 98.66%: 100%|██████████| 572/572 [00:06<00:00, 82.84it/s]\n",
            "[17 / 20]   Val: Loss = 0.09348, Accuracy = 97.00%: 100%|██████████| 13/13 [00:00<00:00, 63.50it/s]\n",
            "[18 / 20] Train: Loss = 0.04091, Accuracy = 98.71%: 100%|██████████| 572/572 [00:06<00:00, 83.44it/s]\n",
            "[18 / 20]   Val: Loss = 0.09483, Accuracy = 97.04%: 100%|██████████| 13/13 [00:00<00:00, 65.78it/s]\n",
            "[19 / 20] Train: Loss = 0.03904, Accuracy = 98.78%: 100%|██████████| 572/572 [00:07<00:00, 81.64it/s]\n",
            "[19 / 20]   Val: Loss = 0.09463, Accuracy = 97.05%: 100%|██████████| 13/13 [00:00<00:00, 63.07it/s]\n",
            "[20 / 20] Train: Loss = 0.03740, Accuracy = 98.83%: 100%|██████████| 572/572 [00:07<00:00, 80.68it/s]\n",
            "[20 / 20]   Val: Loss = 0.09510, Accuracy = 97.03%: 100%|██████████| 13/13 [00:00<00:00, 60.90it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "2Ne_8f24h8kg"
      },
      "source": [
        "**Задание** Оцените качество модели на тестовой выборке. Обратите внимание, вовсе не обязательно ограничиваться векторами из урезанной матрицы - вполне могут найтись слова в тесте, которых не было в трейне и для которых есть эмбеддинги.\n",
        "\n",
        "Добейтесь качества лучше прошлых моделей."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "HPUuAPGhEGVR",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        },
        "outputId": "981a919c-49fc-4d52-bb1c-b2d3fa3ebfc9"
      },
      "source": [
        "test_loss, test_acc = do_epoch(model2, criterion, data=(X_test, y_test), batch_size=512)\n",
        "\n",
        "print()\n",
        "print(\"LSTM with pretrained embeddings test accuracy = {}\".format(test_acc))"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "      Loss = 0.09734, Accuracy = 97.01%: 100%|██████████| 28/28 [00:00<00:00, 52.10it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "LSTM with pretrained embeddings test accuracy = 0.9701069649429027\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    }
  ]
}